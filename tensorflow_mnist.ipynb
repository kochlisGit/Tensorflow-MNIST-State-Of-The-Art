{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importin Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Train data: Shape=(60000, 28, 28), Min=0, Max=255, Test Shape: (10000, 28, 28), Targets: [0 1 2 3 4 5 6 7 8 9]'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "f'Train data: Shape={x_train.shape}, Min={np.min(x_train)}, Max={np.max(x_train)}, Test Shape: {x_test.shape}, Targets: {np.unique(y_train)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'Train data: Shape=(60000, 28, 28, 1), Min=0.0, Max=1.0, Test Shape: (10000, 28, 28, 1), Targets: (60000, 10)'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, axis=-1)/255.0\n",
    "x_test = np.expand_dims(x_test, axis=-1)/255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "f'Train data: Shape={x_train.shape}, Min={np.min(x_train)}, Max={np.max(x_train)}, Test Shape: {x_test.shape}, Targets: {y_train.shape}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Custom LeNet CNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_rotation_1 (RandomRo  (None, 28, 28, 1)        0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 26, 26, 32)        288       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 26, 26, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 24, 24, 32)        9216      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 24, 24, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 12, 12, 32)        0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 10, 10, 64)        18432     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 10, 10, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 8, 8, 64)          36864     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 2, 2, 128)         204928    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403,722\n",
      "Trainable params: 403,338\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=x_train.shape[1:]),\n",
    "    tf.keras.layers.RandomRotation(factor=0.1),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='valid', activation='gelu', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='valid', activation='gelu', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=2, padding='same', activation='gelu'),\n",
    "    tf.keras.layers.Dropout(rate=0.4),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='gelu', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='gelu', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same', activation='gelu'),\n",
    "    tf.keras.layers.Dropout(rate=0.4),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same', activation='gelu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.9438\n",
      "Epoch 1: val_loss improved from inf to 0.55356, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 198s 104ms/step - loss: 0.6777 - accuracy: 0.9438 - val_loss: 0.5536 - val_accuracy: 0.9863\n",
      "Epoch 2/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.9810\n",
      "Epoch 2: val_loss improved from 0.55356 to 0.53308, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.5714 - accuracy: 0.9810 - val_loss: 0.5331 - val_accuracy: 0.9920\n",
      "Epoch 3/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.9852\n",
      "Epoch 3: val_loss improved from 0.53308 to 0.53013, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 186s 99ms/step - loss: 0.5563 - accuracy: 0.9852 - val_loss: 0.5301 - val_accuracy: 0.9918\n",
      "Epoch 4/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.9881\n",
      "Epoch 4: val_loss did not improve from 0.53013\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5459 - accuracy: 0.9881 - val_loss: 0.5308 - val_accuracy: 0.9932\n",
      "Epoch 5/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.9894\n",
      "Epoch 5: val_loss improved from 0.53013 to 0.52746, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5404 - accuracy: 0.9894 - val_loss: 0.5275 - val_accuracy: 0.9924\n",
      "Epoch 6/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.9901\n",
      "Epoch 6: val_loss did not improve from 0.52746\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5369 - accuracy: 0.9901 - val_loss: 0.5298 - val_accuracy: 0.9927\n",
      "Epoch 7/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5340 - accuracy: 0.9908\n",
      "Epoch 7: val_loss improved from 0.52746 to 0.52438, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5340 - accuracy: 0.9908 - val_loss: 0.5244 - val_accuracy: 0.9943\n",
      "Epoch 8/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.9917\n",
      "Epoch 8: val_loss did not improve from 0.52438\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5316 - accuracy: 0.9917 - val_loss: 0.5257 - val_accuracy: 0.9938\n",
      "Epoch 9/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.9921\n",
      "Epoch 9: val_loss did not improve from 0.52438\n",
      "1875/1875 [==============================] - 194s 104ms/step - loss: 0.5301 - accuracy: 0.9921 - val_loss: 0.5285 - val_accuracy: 0.9939\n",
      "Epoch 10/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.9923\n",
      "Epoch 10: val_loss did not improve from 0.52438\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5289 - accuracy: 0.9923 - val_loss: 0.5279 - val_accuracy: 0.9934\n",
      "Epoch 11/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.9929\n",
      "Epoch 11: val_loss improved from 0.52438 to 0.52280, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 194s 104ms/step - loss: 0.5271 - accuracy: 0.9929 - val_loss: 0.5228 - val_accuracy: 0.9948\n",
      "Epoch 12/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.9933\n",
      "Epoch 12: val_loss improved from 0.52280 to 0.52251, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 194s 104ms/step - loss: 0.5256 - accuracy: 0.9933 - val_loss: 0.5225 - val_accuracy: 0.9945\n",
      "Epoch 13/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.9942\n",
      "Epoch 13: val_loss did not improve from 0.52251\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5245 - accuracy: 0.9942 - val_loss: 0.5249 - val_accuracy: 0.9943\n",
      "Epoch 14/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.9939\n",
      "Epoch 14: val_loss did not improve from 0.52251\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5240 - accuracy: 0.9939 - val_loss: 0.5245 - val_accuracy: 0.9951\n",
      "Epoch 15/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9941\n",
      "Epoch 15: val_loss improved from 0.52251 to 0.51980, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5235 - accuracy: 0.9941 - val_loss: 0.5198 - val_accuracy: 0.9949\n",
      "Epoch 16/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.9943\n",
      "Epoch 16: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5224 - accuracy: 0.9943 - val_loss: 0.5252 - val_accuracy: 0.9944\n",
      "Epoch 17/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.9944\n",
      "Epoch 17: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5222 - accuracy: 0.9944 - val_loss: 0.5226 - val_accuracy: 0.9944\n",
      "Epoch 18/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.9946\n",
      "Epoch 18: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 200s 106ms/step - loss: 0.5211 - accuracy: 0.9946 - val_loss: 0.5224 - val_accuracy: 0.9943\n",
      "Epoch 19/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.9948\n",
      "Epoch 19: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5209 - accuracy: 0.9948 - val_loss: 0.5205 - val_accuracy: 0.9949\n",
      "Epoch 20/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.9949\n",
      "Epoch 20: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5200 - accuracy: 0.9949 - val_loss: 0.5206 - val_accuracy: 0.9948\n",
      "Epoch 21/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.9950\n",
      "Epoch 21: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5198 - accuracy: 0.9950 - val_loss: 0.5232 - val_accuracy: 0.9944\n",
      "Epoch 22/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.9952\n",
      "Epoch 22: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5189 - accuracy: 0.9952 - val_loss: 0.5202 - val_accuracy: 0.9960\n",
      "Epoch 23/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.9953\n",
      "Epoch 23: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5183 - accuracy: 0.9953 - val_loss: 0.5203 - val_accuracy: 0.9953\n",
      "Epoch 24/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.9956\n",
      "Epoch 24: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5180 - accuracy: 0.9956 - val_loss: 0.5217 - val_accuracy: 0.9946\n",
      "Epoch 25/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.9958\n",
      "Epoch 25: val_loss did not improve from 0.51980\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5176 - accuracy: 0.9958 - val_loss: 0.5203 - val_accuracy: 0.9956\n",
      "Epoch 26/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.9960\n",
      "Epoch 26: val_loss improved from 0.51980 to 0.51973, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5172 - accuracy: 0.9960 - val_loss: 0.5197 - val_accuracy: 0.9955\n",
      "Epoch 27/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.9958\n",
      "Epoch 27: val_loss did not improve from 0.51973\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5173 - accuracy: 0.9958 - val_loss: 0.5219 - val_accuracy: 0.9938\n",
      "Epoch 28/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.9959\n",
      "Epoch 28: val_loss improved from 0.51973 to 0.51809, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5171 - accuracy: 0.9959 - val_loss: 0.5181 - val_accuracy: 0.9951\n",
      "Epoch 29/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.9964\n",
      "Epoch 29: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5165 - accuracy: 0.9964 - val_loss: 0.5195 - val_accuracy: 0.9957\n",
      "Epoch 30/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.9966\n",
      "Epoch 30: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5156 - accuracy: 0.9966 - val_loss: 0.5201 - val_accuracy: 0.9949\n",
      "Epoch 31/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9960\n",
      "Epoch 31: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5163 - accuracy: 0.9960 - val_loss: 0.5199 - val_accuracy: 0.9948\n",
      "Epoch 32/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.9962\n",
      "Epoch 32: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5156 - accuracy: 0.9962 - val_loss: 0.5205 - val_accuracy: 0.9949\n",
      "Epoch 33/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.9964\n",
      "Epoch 33: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5153 - accuracy: 0.9964 - val_loss: 0.5211 - val_accuracy: 0.9955\n",
      "Epoch 34/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5148 - accuracy: 0.9967\n",
      "Epoch 34: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5148 - accuracy: 0.9967 - val_loss: 0.5189 - val_accuracy: 0.9951\n",
      "Epoch 35/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.9966\n",
      "Epoch 35: val_loss did not improve from 0.51809\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5145 - accuracy: 0.9966 - val_loss: 0.5199 - val_accuracy: 0.9951\n",
      "Epoch 36/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.9969\n",
      "Epoch 36: val_loss improved from 0.51809 to 0.51770, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5141 - accuracy: 0.9969 - val_loss: 0.5177 - val_accuracy: 0.9951\n",
      "Epoch 37/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.9966\n",
      "Epoch 37: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5145 - accuracy: 0.9966 - val_loss: 0.5184 - val_accuracy: 0.9956\n",
      "Epoch 38/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.9968\n",
      "Epoch 38: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5139 - accuracy: 0.9968 - val_loss: 0.5178 - val_accuracy: 0.9951\n",
      "Epoch 39/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9967\n",
      "Epoch 39: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 200s 106ms/step - loss: 0.5138 - accuracy: 0.9967 - val_loss: 0.5182 - val_accuracy: 0.9949\n",
      "Epoch 40/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.9968\n",
      "Epoch 40: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5139 - accuracy: 0.9968 - val_loss: 0.5187 - val_accuracy: 0.9942\n",
      "Epoch 41/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.9966\n",
      "Epoch 41: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5143 - accuracy: 0.9966 - val_loss: 0.5180 - val_accuracy: 0.9946\n",
      "Epoch 42/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9969\n",
      "Epoch 42: val_loss did not improve from 0.51770\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.5132 - accuracy: 0.9969 - val_loss: 0.5186 - val_accuracy: 0.9947\n",
      "Epoch 43/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.9973\n",
      "Epoch 43: val_loss improved from 0.51770 to 0.51716, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 207s 110ms/step - loss: 0.5130 - accuracy: 0.9973 - val_loss: 0.5172 - val_accuracy: 0.9948\n",
      "Epoch 44/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.9970\n",
      "Epoch 44: val_loss did not improve from 0.51716\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.5130 - accuracy: 0.9970 - val_loss: 0.5177 - val_accuracy: 0.9944\n",
      "Epoch 45/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.9965\n",
      "Epoch 45: val_loss did not improve from 0.51716\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5135 - accuracy: 0.9965 - val_loss: 0.5176 - val_accuracy: 0.9953\n",
      "Epoch 46/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9971\n",
      "Epoch 46: val_loss did not improve from 0.51716\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5127 - accuracy: 0.9971 - val_loss: 0.5182 - val_accuracy: 0.9946\n",
      "Epoch 47/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.9975\n",
      "Epoch 47: val_loss improved from 0.51716 to 0.51636, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5121 - accuracy: 0.9975 - val_loss: 0.5164 - val_accuracy: 0.9953\n",
      "Epoch 48/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.9972\n",
      "Epoch 48: val_loss improved from 0.51636 to 0.51615, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5125 - accuracy: 0.9972 - val_loss: 0.5162 - val_accuracy: 0.9951\n",
      "Epoch 49/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.9973\n",
      "Epoch 49: val_loss improved from 0.51615 to 0.51546, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5121 - accuracy: 0.9973 - val_loss: 0.5155 - val_accuracy: 0.9960\n",
      "Epoch 50/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.9974\n",
      "Epoch 50: val_loss did not improve from 0.51546\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5119 - accuracy: 0.9974 - val_loss: 0.5176 - val_accuracy: 0.9949\n",
      "Epoch 51/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5113 - accuracy: 0.9976\n",
      "Epoch 51: val_loss did not improve from 0.51546\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5113 - accuracy: 0.9976 - val_loss: 0.5180 - val_accuracy: 0.9951\n",
      "Epoch 52/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.9972\n",
      "Epoch 52: val_loss improved from 0.51546 to 0.51530, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5118 - accuracy: 0.9972 - val_loss: 0.5153 - val_accuracy: 0.9957\n",
      "Epoch 53/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.9972\n",
      "Epoch 53: val_loss did not improve from 0.51530\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5120 - accuracy: 0.9972 - val_loss: 0.5163 - val_accuracy: 0.9951\n",
      "Epoch 54/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5116 - accuracy: 0.9971\n",
      "Epoch 54: val_loss did not improve from 0.51530\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5116 - accuracy: 0.9971 - val_loss: 0.5181 - val_accuracy: 0.9952\n",
      "Epoch 55/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9975\n",
      "Epoch 55: val_loss did not improve from 0.51530\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5112 - accuracy: 0.9975 - val_loss: 0.5156 - val_accuracy: 0.9960\n",
      "Epoch 56/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9975\n",
      "Epoch 56: val_loss improved from 0.51530 to 0.51475, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5112 - accuracy: 0.9975 - val_loss: 0.5147 - val_accuracy: 0.9958\n",
      "Epoch 57/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.9979\n",
      "Epoch 57: val_loss did not improve from 0.51475\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5107 - accuracy: 0.9979 - val_loss: 0.5154 - val_accuracy: 0.9955\n",
      "Epoch 58/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.9972\n",
      "Epoch 58: val_loss did not improve from 0.51475\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5114 - accuracy: 0.9972 - val_loss: 0.5157 - val_accuracy: 0.9952\n",
      "Epoch 59/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9980\n",
      "Epoch 59: val_loss did not improve from 0.51475\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5102 - accuracy: 0.9980 - val_loss: 0.5156 - val_accuracy: 0.9952\n",
      "Epoch 60/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.9977\n",
      "Epoch 60: val_loss improved from 0.51475 to 0.51407, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5104 - accuracy: 0.9977 - val_loss: 0.5141 - val_accuracy: 0.9951\n",
      "Epoch 61/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.9976\n",
      "Epoch 61: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5105 - accuracy: 0.9976 - val_loss: 0.5145 - val_accuracy: 0.9954\n",
      "Epoch 62/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.9976\n",
      "Epoch 62: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5104 - accuracy: 0.9976 - val_loss: 0.5141 - val_accuracy: 0.9959\n",
      "Epoch 63/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.9977\n",
      "Epoch 63: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.5104 - accuracy: 0.9977 - val_loss: 0.5150 - val_accuracy: 0.9959\n",
      "Epoch 64/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.9980\n",
      "Epoch 64: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5099 - accuracy: 0.9980 - val_loss: 0.5154 - val_accuracy: 0.9953\n",
      "Epoch 65/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9978\n",
      "Epoch 65: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5102 - accuracy: 0.9978 - val_loss: 0.5144 - val_accuracy: 0.9952\n",
      "Epoch 66/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.9979\n",
      "Epoch 66: val_loss did not improve from 0.51407\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5101 - accuracy: 0.9979 - val_loss: 0.5143 - val_accuracy: 0.9957\n",
      "Epoch 67/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.9978\n",
      "Epoch 67: val_loss improved from 0.51407 to 0.51397, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5100 - accuracy: 0.9978 - val_loss: 0.5140 - val_accuracy: 0.9962\n",
      "Epoch 68/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9977\n",
      "Epoch 68: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5102 - accuracy: 0.9977 - val_loss: 0.5154 - val_accuracy: 0.9949\n",
      "Epoch 69/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9977\n",
      "Epoch 69: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5102 - accuracy: 0.9977 - val_loss: 0.5147 - val_accuracy: 0.9951\n",
      "Epoch 70/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.9983\n",
      "Epoch 70: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5091 - accuracy: 0.9983 - val_loss: 0.5155 - val_accuracy: 0.9951\n",
      "Epoch 71/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.9978\n",
      "Epoch 71: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5098 - accuracy: 0.9978 - val_loss: 0.5141 - val_accuracy: 0.9955\n",
      "Epoch 72/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.9979\n",
      "Epoch 72: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5095 - accuracy: 0.9979 - val_loss: 0.5158 - val_accuracy: 0.9953\n",
      "Epoch 73/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.9978\n",
      "Epoch 73: val_loss did not improve from 0.51397\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5094 - accuracy: 0.9978 - val_loss: 0.5140 - val_accuracy: 0.9953\n",
      "Epoch 74/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.9980\n",
      "Epoch 74: val_loss improved from 0.51397 to 0.51353, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5095 - accuracy: 0.9980 - val_loss: 0.5135 - val_accuracy: 0.9958\n",
      "Epoch 75/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.9979\n",
      "Epoch 75: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5090 - accuracy: 0.9979 - val_loss: 0.5142 - val_accuracy: 0.9953\n",
      "Epoch 76/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.9981\n",
      "Epoch 76: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5092 - accuracy: 0.9981 - val_loss: 0.5135 - val_accuracy: 0.9955\n",
      "Epoch 77/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.9980\n",
      "Epoch 77: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5091 - accuracy: 0.9980 - val_loss: 0.5140 - val_accuracy: 0.9954\n",
      "Epoch 78/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9982\n",
      "Epoch 78: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5088 - accuracy: 0.9982 - val_loss: 0.5151 - val_accuracy: 0.9952\n",
      "Epoch 79/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.9980\n",
      "Epoch 79: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 187s 100ms/step - loss: 0.5087 - accuracy: 0.9980 - val_loss: 0.5135 - val_accuracy: 0.9956\n",
      "Epoch 80/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.9980\n",
      "Epoch 80: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5085 - accuracy: 0.9980 - val_loss: 0.5140 - val_accuracy: 0.9955\n",
      "Epoch 81/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.9981\n",
      "Epoch 81: val_loss did not improve from 0.51353\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5086 - accuracy: 0.9981 - val_loss: 0.5149 - val_accuracy: 0.9948\n",
      "Epoch 82/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9983\n",
      "Epoch 82: val_loss improved from 0.51353 to 0.51284, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5088 - accuracy: 0.9983 - val_loss: 0.5128 - val_accuracy: 0.9955\n",
      "Epoch 83/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.9981\n",
      "Epoch 83: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5085 - accuracy: 0.9981 - val_loss: 0.5132 - val_accuracy: 0.9957\n",
      "Epoch 84/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9986\n",
      "Epoch 84: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5078 - accuracy: 0.9986 - val_loss: 0.5134 - val_accuracy: 0.9957\n",
      "Epoch 85/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.9984\n",
      "Epoch 85: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5082 - accuracy: 0.9984 - val_loss: 0.5133 - val_accuracy: 0.9956\n",
      "Epoch 86/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.9980\n",
      "Epoch 86: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5084 - accuracy: 0.9980 - val_loss: 0.5143 - val_accuracy: 0.9945\n",
      "Epoch 87/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9980\n",
      "Epoch 87: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5088 - accuracy: 0.9980 - val_loss: 0.5136 - val_accuracy: 0.9956\n",
      "Epoch 88/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.9984\n",
      "Epoch 88: val_loss did not improve from 0.51284\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5079 - accuracy: 0.9984 - val_loss: 0.5139 - val_accuracy: 0.9951\n",
      "Epoch 89/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9980\n",
      "Epoch 89: val_loss improved from 0.51284 to 0.51283, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5088 - accuracy: 0.9980 - val_loss: 0.5128 - val_accuracy: 0.9952\n",
      "Epoch 90/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.9983\n",
      "Epoch 90: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5084 - accuracy: 0.9983 - val_loss: 0.5130 - val_accuracy: 0.9959\n",
      "Epoch 91/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.9981\n",
      "Epoch 91: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 192s 103ms/step - loss: 0.5081 - accuracy: 0.9981 - val_loss: 0.5134 - val_accuracy: 0.9955\n",
      "Epoch 92/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.9984\n",
      "Epoch 92: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5079 - accuracy: 0.9984 - val_loss: 0.5131 - val_accuracy: 0.9954\n",
      "Epoch 93/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.9985\n",
      "Epoch 93: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5079 - accuracy: 0.9985 - val_loss: 0.5140 - val_accuracy: 0.9954\n",
      "Epoch 94/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.9981\n",
      "Epoch 94: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5082 - accuracy: 0.9981 - val_loss: 0.5131 - val_accuracy: 0.9958\n",
      "Epoch 95/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.9983\n",
      "Epoch 95: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5077 - accuracy: 0.9983 - val_loss: 0.5133 - val_accuracy: 0.9956\n",
      "Epoch 96/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.9981\n",
      "Epoch 96: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5081 - accuracy: 0.9981 - val_loss: 0.5147 - val_accuracy: 0.9954\n",
      "Epoch 97/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9984\n",
      "Epoch 97: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5078 - accuracy: 0.9984 - val_loss: 0.5140 - val_accuracy: 0.9954\n",
      "Epoch 98/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.9982\n",
      "Epoch 98: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5080 - accuracy: 0.9982 - val_loss: 0.5132 - val_accuracy: 0.9958\n",
      "Epoch 99/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.9984\n",
      "Epoch 99: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 187s 100ms/step - loss: 0.5076 - accuracy: 0.9984 - val_loss: 0.5132 - val_accuracy: 0.9958\n",
      "Epoch 100/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.9985\n",
      "Epoch 100: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5077 - accuracy: 0.9985 - val_loss: 0.5131 - val_accuracy: 0.9956\n",
      "Epoch 101/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9982\n",
      "Epoch 101: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 192s 102ms/step - loss: 0.5078 - accuracy: 0.9982 - val_loss: 0.5142 - val_accuracy: 0.9952\n",
      "Epoch 102/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.9984\n",
      "Epoch 102: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5073 - accuracy: 0.9984 - val_loss: 0.5136 - val_accuracy: 0.9951\n",
      "Epoch 103/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.9985\n",
      "Epoch 103: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5074 - accuracy: 0.9985 - val_loss: 0.5134 - val_accuracy: 0.9953\n",
      "Epoch 104/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.9985\n",
      "Epoch 104: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5073 - accuracy: 0.9985 - val_loss: 0.5132 - val_accuracy: 0.9959\n",
      "Epoch 105/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.9984\n",
      "Epoch 105: val_loss did not improve from 0.51283\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5074 - accuracy: 0.9984 - val_loss: 0.5130 - val_accuracy: 0.9952\n",
      "Epoch 106/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9986\n",
      "Epoch 106: val_loss improved from 0.51283 to 0.51226, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5070 - accuracy: 0.9986 - val_loss: 0.5123 - val_accuracy: 0.9959\n",
      "Epoch 107/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9985\n",
      "Epoch 107: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5070 - accuracy: 0.9985 - val_loss: 0.5136 - val_accuracy: 0.9957\n",
      "Epoch 108/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.9986\n",
      "Epoch 108: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 194s 103ms/step - loss: 0.5072 - accuracy: 0.9986 - val_loss: 0.5135 - val_accuracy: 0.9951\n",
      "Epoch 109/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.9986\n",
      "Epoch 109: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 194s 103ms/step - loss: 0.5069 - accuracy: 0.9986 - val_loss: 0.5134 - val_accuracy: 0.9954\n",
      "Epoch 110/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.9983\n",
      "Epoch 110: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 194s 103ms/step - loss: 0.5075 - accuracy: 0.9983 - val_loss: 0.5140 - val_accuracy: 0.9946\n",
      "Epoch 111/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.9985\n",
      "Epoch 111: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 194s 104ms/step - loss: 0.5072 - accuracy: 0.9985 - val_loss: 0.5128 - val_accuracy: 0.9955\n",
      "Epoch 112/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9984\n",
      "Epoch 112: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5071 - accuracy: 0.9984 - val_loss: 0.5127 - val_accuracy: 0.9957\n",
      "Epoch 113/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.9986\n",
      "Epoch 113: val_loss did not improve from 0.51226\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5068 - accuracy: 0.9986 - val_loss: 0.5130 - val_accuracy: 0.9955\n",
      "Epoch 114/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.9987\n",
      "Epoch 114: val_loss improved from 0.51226 to 0.51188, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5066 - accuracy: 0.9987 - val_loss: 0.5119 - val_accuracy: 0.9962\n",
      "Epoch 115/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9984\n",
      "Epoch 115: val_loss improved from 0.51188 to 0.51176, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.5070 - accuracy: 0.9984 - val_loss: 0.5118 - val_accuracy: 0.9964\n",
      "Epoch 116/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.9987\n",
      "Epoch 116: val_loss did not improve from 0.51176\n",
      "1875/1875 [==============================] - 192s 102ms/step - loss: 0.5066 - accuracy: 0.9987 - val_loss: 0.5124 - val_accuracy: 0.9957\n",
      "Epoch 117/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9987\n",
      "Epoch 117: val_loss improved from 0.51176 to 0.51108, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5065 - accuracy: 0.9987 - val_loss: 0.5111 - val_accuracy: 0.9963\n",
      "Epoch 118/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.9987\n",
      "Epoch 118: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 191s 102ms/step - loss: 0.5067 - accuracy: 0.9987 - val_loss: 0.5122 - val_accuracy: 0.9957\n",
      "Epoch 119/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.9984\n",
      "Epoch 119: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5069 - accuracy: 0.9984 - val_loss: 0.5125 - val_accuracy: 0.9952\n",
      "Epoch 120/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9987\n",
      "Epoch 120: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5065 - accuracy: 0.9987 - val_loss: 0.5129 - val_accuracy: 0.9956\n",
      "Epoch 121/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9986\n",
      "Epoch 121: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 190s 102ms/step - loss: 0.5064 - accuracy: 0.9986 - val_loss: 0.5129 - val_accuracy: 0.9950\n",
      "Epoch 122/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.9985\n",
      "Epoch 122: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.5067 - accuracy: 0.9985 - val_loss: 0.5115 - val_accuracy: 0.9960\n",
      "Epoch 123/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.9983\n",
      "Epoch 123: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5069 - accuracy: 0.9983 - val_loss: 0.5128 - val_accuracy: 0.9953\n",
      "Epoch 124/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.9986\n",
      "Epoch 124: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.5063 - accuracy: 0.9986 - val_loss: 0.5130 - val_accuracy: 0.9952\n",
      "Epoch 125/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9987\n",
      "Epoch 125: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5064 - accuracy: 0.9987 - val_loss: 0.5125 - val_accuracy: 0.9954\n",
      "Epoch 126/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.9985\n",
      "Epoch 126: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 190s 101ms/step - loss: 0.5064 - accuracy: 0.9985 - val_loss: 0.5125 - val_accuracy: 0.9954\n",
      "Epoch 127/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.9985\n",
      "Epoch 127: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 192s 103ms/step - loss: 0.5063 - accuracy: 0.9985 - val_loss: 0.5120 - val_accuracy: 0.9957\n",
      "Epoch 128/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.9987\n",
      "Epoch 128: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 208s 111ms/step - loss: 0.5061 - accuracy: 0.9987 - val_loss: 0.5136 - val_accuracy: 0.9952\n",
      "Epoch 129/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.9988\n",
      "Epoch 129: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 203s 108ms/step - loss: 0.5062 - accuracy: 0.9988 - val_loss: 0.5116 - val_accuracy: 0.9959\n",
      "Epoch 130/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.9986\n",
      "Epoch 130: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.5060 - accuracy: 0.9986 - val_loss: 0.5137 - val_accuracy: 0.9954\n",
      "Epoch 131/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.9984\n",
      "Epoch 131: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 214s 114ms/step - loss: 0.5066 - accuracy: 0.9984 - val_loss: 0.5119 - val_accuracy: 0.9957\n",
      "Epoch 132/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.9984\n",
      "Epoch 132: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.5067 - accuracy: 0.9984 - val_loss: 0.5131 - val_accuracy: 0.9952\n",
      "Epoch 133/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9985\n",
      "Epoch 133: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 216s 115ms/step - loss: 0.5065 - accuracy: 0.9985 - val_loss: 0.5129 - val_accuracy: 0.9952\n",
      "Epoch 134/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.9984\n",
      "Epoch 134: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 220s 117ms/step - loss: 0.5067 - accuracy: 0.9984 - val_loss: 0.5132 - val_accuracy: 0.9958\n",
      "Epoch 135/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.9987\n",
      "Epoch 135: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 216s 115ms/step - loss: 0.5062 - accuracy: 0.9987 - val_loss: 0.5116 - val_accuracy: 0.9953\n",
      "Epoch 136/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.9987\n",
      "Epoch 136: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 221s 118ms/step - loss: 0.5060 - accuracy: 0.9987 - val_loss: 0.5126 - val_accuracy: 0.9957\n",
      "Epoch 137/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.9988\n",
      "Epoch 137: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 216s 115ms/step - loss: 0.5060 - accuracy: 0.9988 - val_loss: 0.5121 - val_accuracy: 0.9956\n",
      "Epoch 138/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.9989\n",
      "Epoch 138: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 216s 115ms/step - loss: 0.5060 - accuracy: 0.9989 - val_loss: 0.5126 - val_accuracy: 0.9956\n",
      "Epoch 139/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.9987\n",
      "Epoch 139: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 220s 117ms/step - loss: 0.5059 - accuracy: 0.9987 - val_loss: 0.5143 - val_accuracy: 0.9948\n",
      "Epoch 140/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.9987\n",
      "Epoch 140: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 217s 116ms/step - loss: 0.5058 - accuracy: 0.9987 - val_loss: 0.5131 - val_accuracy: 0.9955\n",
      "Epoch 141/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.9987\n",
      "Epoch 141: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5059 - accuracy: 0.9987 - val_loss: 0.5123 - val_accuracy: 0.9954\n",
      "Epoch 142/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.9988\n",
      "Epoch 142: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 204s 109ms/step - loss: 0.5057 - accuracy: 0.9988 - val_loss: 0.5120 - val_accuracy: 0.9960\n",
      "Epoch 143/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9990\n",
      "Epoch 143: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5051 - accuracy: 0.9990 - val_loss: 0.5112 - val_accuracy: 0.9960\n",
      "Epoch 144/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.9987\n",
      "Epoch 144: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5058 - accuracy: 0.9987 - val_loss: 0.5118 - val_accuracy: 0.9958\n",
      "Epoch 145/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.9986\n",
      "Epoch 145: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5059 - accuracy: 0.9986 - val_loss: 0.5117 - val_accuracy: 0.9954\n",
      "Epoch 146/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.9987\n",
      "Epoch 146: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5060 - accuracy: 0.9987 - val_loss: 0.5116 - val_accuracy: 0.9961\n",
      "Epoch 147/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.9988\n",
      "Epoch 147: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5057 - accuracy: 0.9988 - val_loss: 0.5123 - val_accuracy: 0.9957\n",
      "Epoch 148/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.9986\n",
      "Epoch 148: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5058 - accuracy: 0.9986 - val_loss: 0.5122 - val_accuracy: 0.9959\n",
      "Epoch 149/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.9987\n",
      "Epoch 149: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5059 - accuracy: 0.9987 - val_loss: 0.5113 - val_accuracy: 0.9958\n",
      "Epoch 150/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9989\n",
      "Epoch 150: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5056 - accuracy: 0.9989 - val_loss: 0.5124 - val_accuracy: 0.9958\n",
      "Epoch 151/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9988\n",
      "Epoch 151: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5053 - accuracy: 0.9988 - val_loss: 0.5124 - val_accuracy: 0.9954\n",
      "Epoch 152/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.9987\n",
      "Epoch 152: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5055 - accuracy: 0.9987 - val_loss: 0.5126 - val_accuracy: 0.9958\n",
      "Epoch 153/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9988\n",
      "Epoch 153: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5056 - accuracy: 0.9988 - val_loss: 0.5111 - val_accuracy: 0.9957\n",
      "Epoch 154/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9989\n",
      "Epoch 154: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5053 - accuracy: 0.9989 - val_loss: 0.5113 - val_accuracy: 0.9961\n",
      "Epoch 155/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9989\n",
      "Epoch 155: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 205s 109ms/step - loss: 0.5053 - accuracy: 0.9989 - val_loss: 0.5119 - val_accuracy: 0.9956\n",
      "Epoch 156/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9988\n",
      "Epoch 156: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 209s 112ms/step - loss: 0.5051 - accuracy: 0.9988 - val_loss: 0.5127 - val_accuracy: 0.9955\n",
      "Epoch 157/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.9990\n",
      "Epoch 157: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5052 - accuracy: 0.9990 - val_loss: 0.5124 - val_accuracy: 0.9949\n",
      "Epoch 158/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9990\n",
      "Epoch 158: val_loss did not improve from 0.51108\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5053 - accuracy: 0.9990 - val_loss: 0.5116 - val_accuracy: 0.9956\n",
      "Epoch 159/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9987\n",
      "Epoch 159: val_loss improved from 0.51108 to 0.51063, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5056 - accuracy: 0.9987 - val_loss: 0.5106 - val_accuracy: 0.9959\n",
      "Epoch 160/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9990\n",
      "Epoch 160: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5053 - accuracy: 0.9990 - val_loss: 0.5118 - val_accuracy: 0.9953\n",
      "Epoch 161/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9990\n",
      "Epoch 161: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5051 - accuracy: 0.9990 - val_loss: 0.5113 - val_accuracy: 0.9957\n",
      "Epoch 162/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.9989\n",
      "Epoch 162: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5052 - accuracy: 0.9989 - val_loss: 0.5120 - val_accuracy: 0.9956\n",
      "Epoch 163/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9990\n",
      "Epoch 163: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5051 - accuracy: 0.9990 - val_loss: 0.5115 - val_accuracy: 0.9956\n",
      "Epoch 164/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.9988\n",
      "Epoch 164: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5052 - accuracy: 0.9988 - val_loss: 0.5107 - val_accuracy: 0.9960\n",
      "Epoch 165/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9987\n",
      "Epoch 165: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5056 - accuracy: 0.9987 - val_loss: 0.5115 - val_accuracy: 0.9957\n",
      "Epoch 166/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.9989\n",
      "Epoch 166: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5055 - accuracy: 0.9989 - val_loss: 0.5119 - val_accuracy: 0.9956\n",
      "Epoch 167/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9991\n",
      "Epoch 167: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5051 - accuracy: 0.9991 - val_loss: 0.5115 - val_accuracy: 0.9963\n",
      "Epoch 168/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9990\n",
      "Epoch 168: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5051 - accuracy: 0.9990 - val_loss: 0.5119 - val_accuracy: 0.9955\n",
      "Epoch 169/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.9987\n",
      "Epoch 169: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5055 - accuracy: 0.9987 - val_loss: 0.5118 - val_accuracy: 0.9957\n",
      "Epoch 170/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.9988\n",
      "Epoch 170: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5054 - accuracy: 0.9988 - val_loss: 0.5110 - val_accuracy: 0.9964\n",
      "Epoch 171/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9989\n",
      "Epoch 171: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5051 - accuracy: 0.9989 - val_loss: 0.5112 - val_accuracy: 0.9963\n",
      "Epoch 172/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9990\n",
      "Epoch 172: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5048 - accuracy: 0.9990 - val_loss: 0.5119 - val_accuracy: 0.9959\n",
      "Epoch 173/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5049 - accuracy: 0.9990\n",
      "Epoch 173: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5049 - accuracy: 0.9990 - val_loss: 0.5122 - val_accuracy: 0.9955\n",
      "Epoch 174/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.9990\n",
      "Epoch 174: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5050 - accuracy: 0.9990 - val_loss: 0.5112 - val_accuracy: 0.9962\n",
      "Epoch 175/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9991\n",
      "Epoch 175: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5048 - accuracy: 0.9991 - val_loss: 0.5112 - val_accuracy: 0.9959\n",
      "Epoch 176/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9991\n",
      "Epoch 176: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5044 - accuracy: 0.9991 - val_loss: 0.5126 - val_accuracy: 0.9954\n",
      "Epoch 177/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9988\n",
      "Epoch 177: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.5053 - accuracy: 0.9988 - val_loss: 0.5116 - val_accuracy: 0.9960\n",
      "Epoch 178/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.9988\n",
      "Epoch 178: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5050 - accuracy: 0.9988 - val_loss: 0.5120 - val_accuracy: 0.9958\n",
      "Epoch 179/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.9988\n",
      "Epoch 179: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5050 - accuracy: 0.9988 - val_loss: 0.5122 - val_accuracy: 0.9960\n",
      "Epoch 180/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9989\n",
      "Epoch 180: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 203s 108ms/step - loss: 0.5051 - accuracy: 0.9989 - val_loss: 0.5116 - val_accuracy: 0.9955\n",
      "Epoch 181/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9989\n",
      "Epoch 181: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5051 - accuracy: 0.9989 - val_loss: 0.5110 - val_accuracy: 0.9960\n",
      "Epoch 182/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9991\n",
      "Epoch 182: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5048 - accuracy: 0.9991 - val_loss: 0.5116 - val_accuracy: 0.9958\n",
      "Epoch 183/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9992\n",
      "Epoch 183: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5044 - accuracy: 0.9992 - val_loss: 0.5117 - val_accuracy: 0.9959\n",
      "Epoch 184/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9990\n",
      "Epoch 184: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 196s 104ms/step - loss: 0.5048 - accuracy: 0.9990 - val_loss: 0.5117 - val_accuracy: 0.9956\n",
      "Epoch 185/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.9990\n",
      "Epoch 185: val_loss did not improve from 0.51063\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5048 - accuracy: 0.9990 - val_loss: 0.5115 - val_accuracy: 0.9953\n",
      "Epoch 186/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9991\n",
      "Epoch 186: val_loss improved from 0.51063 to 0.51060, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5046 - accuracy: 0.9991 - val_loss: 0.5106 - val_accuracy: 0.9962\n",
      "Epoch 187/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.9991\n",
      "Epoch 187: val_loss did not improve from 0.51060\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5045 - accuracy: 0.9991 - val_loss: 0.5107 - val_accuracy: 0.9959\n",
      "Epoch 188/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9990\n",
      "Epoch 188: val_loss did not improve from 0.51060\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5047 - accuracy: 0.9990 - val_loss: 0.5109 - val_accuracy: 0.9958\n",
      "Epoch 189/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9991\n",
      "Epoch 189: val_loss improved from 0.51060 to 0.51055, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5046 - accuracy: 0.9991 - val_loss: 0.5105 - val_accuracy: 0.9964\n",
      "Epoch 190/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9989\n",
      "Epoch 190: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.5051 - accuracy: 0.9989 - val_loss: 0.5112 - val_accuracy: 0.9957\n",
      "Epoch 191/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9990\n",
      "Epoch 191: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 200s 107ms/step - loss: 0.5047 - accuracy: 0.9990 - val_loss: 0.5112 - val_accuracy: 0.9959\n",
      "Epoch 192/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9989\n",
      "Epoch 192: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5046 - accuracy: 0.9989 - val_loss: 0.5113 - val_accuracy: 0.9961\n",
      "Epoch 193/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9989\n",
      "Epoch 193: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5047 - accuracy: 0.9989 - val_loss: 0.5112 - val_accuracy: 0.9959\n",
      "Epoch 194/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9992\n",
      "Epoch 194: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5044 - accuracy: 0.9992 - val_loss: 0.5121 - val_accuracy: 0.9957\n",
      "Epoch 195/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9990\n",
      "Epoch 195: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 199s 106ms/step - loss: 0.5046 - accuracy: 0.9990 - val_loss: 0.5114 - val_accuracy: 0.9958\n",
      "Epoch 196/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.9992\n",
      "Epoch 196: val_loss did not improve from 0.51055\n",
      "1875/1875 [==============================] - 198s 105ms/step - loss: 0.5042 - accuracy: 0.9992 - val_loss: 0.5107 - val_accuracy: 0.9962\n",
      "Epoch 197/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.9990\n",
      "Epoch 197: val_loss improved from 0.51055 to 0.51048, saving model to checkpoint\\mnist_cnn_callback.h5\n",
      "1875/1875 [==============================] - 198s 106ms/step - loss: 0.5045 - accuracy: 0.9990 - val_loss: 0.5105 - val_accuracy: 0.9961\n",
      "Epoch 198/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.9990\n",
      "Epoch 198: val_loss did not improve from 0.51048\n",
      "1875/1875 [==============================] - 204s 109ms/step - loss: 0.5046 - accuracy: 0.9990 - val_loss: 0.5109 - val_accuracy: 0.9962\n",
      "Epoch 199/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9992\n",
      "Epoch 199: val_loss did not improve from 0.51048\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.5044 - accuracy: 0.9992 - val_loss: 0.5118 - val_accuracy: 0.9957\n",
      "Epoch 200/200\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.9989\n",
      "Epoch 200: val_loss did not improve from 0.51048\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.5047 - accuracy: 0.9989 - val_loss: 0.5117 - val_accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.Yogi(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='checkpoint/mnist_cnn_callback.h5',\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=50,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='checkpoint',\n",
    "        write_graph=False\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Training Progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "plt.plot(history_dict['loss'])\n",
    "plt.plot(history_dict['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training Loss', 'Validatin Loss'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       980\n",
      "           1       1.00      1.00      1.00      1135\n",
      "           2       1.00      1.00      1.00      1032\n",
      "           3       1.00      1.00      1.00      1010\n",
      "           4       0.99      1.00      0.99       982\n",
      "           5       1.00      1.00      1.00       892\n",
      "           6       1.00      0.99      0.99       958\n",
      "           7       0.99      1.00      0.99      1028\n",
      "           8       1.00      1.00      1.00       974\n",
      "           9       1.00      0.99      0.99      1009\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_actual = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "print(classification_report(y_actual, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'Mean Validation Accuracy = 0.9956'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f'Mean Validation Accuracy = {accuracy_score(y_actual, y_pred)}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}